{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.3 64-bit"
  },
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping libraries.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Sentiment Analisys libraries.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import mtranslate\n",
    "\n",
    "# System libraries.\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "# Lithops.\n",
    "from threading import Thread\n",
    "from lithops.multiprocessing import Pool\n",
    "from lithops import Storage\n",
    "\n",
    "# Http header.\n",
    "header = { 'user-agent':'Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0' }\n",
    "\n",
    "SEARCH_KEY = ''\n",
    "STORAGE = None\n",
    "BUCKET = 'news-bucket'\n",
    "\n",
    "# The higher, more results. The lower, less results.\n",
    "DEFAULT_SEARCH_RESULTS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# WEB SCRAWLER FOR THE WEBSITE WWW.CCMA.CAT   #\n",
    "###############################################\n",
    "def ccma_process_news(link):\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    news_format = {}\n",
    "    try:\n",
    "        r = requests.get(link, headers=header)\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        return 0\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    frame = soup.find(\"div\", class_='span8')\n",
    "    \n",
    "    # Put news link.\n",
    "    news_format['link'] = link\n",
    "\n",
    "    # Get news header.\n",
    "    news_format['title'] = frame.find(\"h1\", class_='titol').text\n",
    "\n",
    "    # Get news starter.\n",
    "    starter = frame.find(\"h2\", class_='entradeta')\n",
    "    if starter is not None:\n",
    "        starter = starter.text\n",
    "    else: starter = ''\n",
    "    news_format['starter'] = starter.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    # Get news date.\n",
    "    news_format['date'] = frame.find(\"time\", class_='data').text.split(\" \")[0]\n",
    "\n",
    "    # Get news paragraphs.\n",
    "    frame = frame.find(\"div\", class_='R-itemNotiCos')\n",
    "    body = ''\n",
    "    for parraph in frame.find_all(\"p\"):\n",
    "        body = body+\" \"+parraph.text\n",
    "    news_format['body'] = body.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    #Get news sentiment analysis.\n",
    "    try:\n",
    "        news_format['sentiment'] = analyzer.polarity_scores(mtranslate.translate(news_format['starter']+'\\n'+news_format['body'],'en','auto'))['compound']\n",
    "    except urllib.error.HTTPError:\n",
    "        return 0\n",
    "        \n",
    "    # Get news total words number.\n",
    "    word_counter = 0\n",
    "    for field in news_format.values():\n",
    "        word_counter += len(str(field).split(\" \"))\n",
    "    news_format['words_number'] = word_counter\n",
    "\n",
    "    # Store the news content to the cloud COS.\n",
    "    storage = Storage()\n",
    "    storage.put_object(bucket='news-bucket', key=SEARCH_KEY+'/ccma/'+news_format['title'].replace(\" \",\"_\")+'.json', body = json.dumps(news_format))\n",
    "\n",
    "    return 1\n",
    "\n",
    "def ccma_get_links():\n",
    "\n",
    "    # Auxiliar variables.\n",
    "    link_to_news = []\n",
    "\n",
    "    # We create HTML parser.\n",
    "    r = requests.get('https://www.ccma.cat/cercador/?text='+SEARCH_KEY+'&profile=noticies&pagina=1', headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Get the number of pages in the website.\n",
    "    pages = soup.find(class_='numeracio')\n",
    "    if pages is None:\n",
    "        pages = 0\n",
    "    else:\n",
    "        pages = pages.text.split(\" \")[3]\n",
    "\n",
    "    # Get the links to the news.\n",
    "    for i in range(int(pages)+1):\n",
    "        r = requests.get('https://www.ccma.cat/cercador/?text='+SEARCH_KEY+'&profile=noticies&pagina='+str(i), headers=header)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        for news in soup.find_all(\"li\", class_='F-llistat-item'):\n",
    "            # We get the link to the news page.\n",
    "            link_to_news.append(\"https://www.ccma.cat\"+news.find(\"a\").get('href'))\n",
    "    \n",
    "    return link_to_news\n",
    "\n",
    "def ccma_query():\n",
    "\n",
    "    link_to_news = ccma_get_links()\n",
    "\n",
    "    # Start cloud multiprocessing.\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(ccma_process_news, link_to_news)\n",
    "    \n",
    "    count = sum(result)\n",
    "\n",
    "    if count == 0:\n",
    "        return \"ccma: no results found.\"\n",
    "    else:\n",
    "        return \"ccma: \"+str(count)+\" results found.\"\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# WEB SCRAWLER FOR THE WEBSITE WWW.DIARIDETARRAGONA.COM   #\n",
    "###########################################################\n",
    "\n",
    "def dtg_process_news(link):\n",
    "     \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    news_format = {}\n",
    "    try:\n",
    "        r = requests.get(link, headers=header)\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        return 0\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    head = soup.find(\"header\", class_='news-header')\n",
    "\n",
    "    # Put news link.\n",
    "    news_format['link'] = link\n",
    "\n",
    "    # Get news header.\n",
    "    news_format['title'] = head.find(\"h1\", class_='news-title').text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    # Get news starter.\n",
    "    starter = head.find(\"div\",class_='news-excerpt')\n",
    "    if starter is not None:\n",
    "        starter = starter.text\n",
    "    else: starter = ''\n",
    "    news_format['starter'] = starter.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    # Get news date.\n",
    "    news_format['date'] = head.find(\"time\", class_='news-date').text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").split(\" \")[0]\n",
    "\n",
    "    # Get news paragraphs.\n",
    "    frame = soup.find(\"div\", class_='news-body')\n",
    "    body = ''\n",
    "    for parraph in frame.find_all(\"p\"):\n",
    "        body = body+\" \"+parraph.text\n",
    "    news_format['body'] = body.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    #Get news sentiment analysis.\n",
    "    try:\n",
    "        news_format['sentiment'] = analyzer.polarity_scores(mtranslate.translate(news_format['starter']+'\\n'+news_format['body'],'en','auto'))['compound']\n",
    "    except urllib.error.HTTPError:\n",
    "        return 0\n",
    "    \n",
    "    # Get news total words number.\n",
    "    word_counter = 0\n",
    "    for field in news_format.values():\n",
    "        word_counter += len(str(field).split(\" \"))\n",
    "    news_format['words_number'] = word_counter\n",
    "\n",
    "    # Store the news content to the cloud COS.\n",
    "    storage = Storage()\n",
    "    storage.put_object(bucket='news-bucket', key=SEARCH_KEY+'/diaridetarragona/'+news_format['title'].replace(\" \",\"_\")+'.json', body = json.dumps(news_format))\n",
    "\n",
    "    return 1\n",
    "\n",
    "def dtg_get_links():\n",
    "\n",
    "    # Auxiliar variables.\n",
    "    link_to_news = []\n",
    "\n",
    "    # We create HTML parser.\n",
    "    r = requests.get('https://www.diaridetarragona.com/ajax/get_search_news.html?viewmore=%2Fajax%2Fget_search_news.html&page=1&size='+str(DEFAULT_SEARCH_RESULTS)+'&search='+SEARCH_KEY, headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Get the links to the news.\n",
    "    for news in soup.find_all(\"div\", class_='news-data'):\n",
    "        # We get the link to the news page.\n",
    "        link_to_news.append(\"https://www.diaridetarragona.com\"+news.find(\"a\").get('href'))\n",
    "\n",
    "    return link_to_news\n",
    "\n",
    "def dtg_query():\n",
    "    \n",
    "    link_to_news = dtg_get_links()\n",
    "\n",
    "    # Start cloud multiprocessing.\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(dtg_process_news, link_to_news)\n",
    "    count = sum(result)\n",
    "    \n",
    "    if count == 0:\n",
    "        return \"DiarideTarragona: no results found.\"\n",
    "    else:\n",
    "        return \"DiarideTarragona: \"+str(count)+\" results found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbc_process_news(link):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    news_format = {}\n",
    "    try:\n",
    "        r = requests.get(link, headers=header)\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        return 0\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # Put news link.\n",
    "    news_format['link'] = link\n",
    "\n",
    "    # Get news header.\n",
    "    title = soup.find(\"div\", class_='title-opening-section')\n",
    "    if title is None:\n",
    "        return 0         #  Case we are threatting an opinion...\n",
    "    news_format['title'] = title.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\\"\",\"\")\n",
    "\n",
    "    # Get news starter.\n",
    "    description = soup.find(\"div\", class_='description')\n",
    "    if description is None:\n",
    "        return 0         # Case we are threatting an interview...\n",
    "    news_format['starter'] = description.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\xa0\",\" \").replace(\"\\\"\",\"\")\n",
    "\n",
    "    # Get news date.\n",
    "    frame = soup.find(\"div\", class_='info-date')\n",
    "    date = frame.find(\"span\", class_='date').text\n",
    "    news_format['date'] = date.replace(\" \",\"\").replace(\"d’\",\"/\").replace(\"de\",\"/\").replace(\"gener\",\"01/\").replace(\"febrer\",\"02/\").replace(\"març\",\"03/\").replace(\"abril\",\"04/\").replace(\"maig\",\"05/\").replace(\"juny\",\"06/\").replace(\"juliol\",\"07/\").replace(\"agost\",\"08/\").replace(\"setembre\",\"09/\").replace(\"octubre\",\"10/\").replace(\"novembre\",\"11/\").replace(\"desembre\",\"12/\")\n",
    "    # Get news paragraphs.\n",
    "    frame = soup.findAll(\"div\", class_='component-html pb-3')[3]\n",
    "    body = ''\n",
    "    for parraph in frame.find_all(\"p\"):\n",
    "        body = body+\" \"+parraph.text\n",
    "    news_format['body'] = body.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\xa0\",\" \").replace(\"\\\"\",\"\")\n",
    "    \n",
    "    #Get news sentiment analysis.\n",
    "    try:\n",
    "        news_format['sentiment'] = analyzer.polarity_scores(mtranslate.translate(news_format['starter']+'\\n'+news_format['body'],'en','auto'))['compound']\n",
    "    except urllib.error.HTTPError:\n",
    "        return 0\n",
    "\n",
    "    # Get news total words number.\n",
    "    word_counter = 0\n",
    "    for field in news_format.values():\n",
    "        word_counter += len(str(field).split(\" \"))\n",
    "    news_format['words_number'] = word_counter\n",
    "\n",
    "    # Store the news content to the cloud COS.\n",
    "    storage = Storage()\n",
    "    storage.put_object(bucket='news-bucket', key=SEARCH_KEY+'/diaridebarcelona/'+news_format['title'].replace(\" \",\"_\")+'.json', body = json.dumps(news_format))\n",
    "\n",
    "    return 1\n",
    "\n",
    "def dbc_get_links():\n",
    "\n",
    "    # Auxiliar variables.\n",
    "    link_to_news = []\n",
    "\n",
    "    # We create HTML parser.\n",
    "    r = requests.get('https://www.diaridebarcelona.cat/search?q='+sys.argv[1], headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Get the number of pages in the website.\n",
    "    frame = soup.find(lambda tag: tag.name == 'li' and tag.get('class') == ['first'])\n",
    "    if frame is not None:\n",
    "        pages = frame.find(\"a\").get('href')\n",
    "        pages = pages.split(\"=\")[-1]\n",
    "    else:\n",
    "        pages = 0  \n",
    "\n",
    "    # Get the links to the news.\n",
    "    for i in range(int(pages)+1):\n",
    "        r = requests.get('https://www.diaridebarcelona.cat/search?q='+SEARCH_KEY+'&start='+str(i), headers=header)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        for news in soup.find_all(class_='col-sm-6 col-lg-3 mb-20px mb-lg-30px'):\n",
    "            # We get the link to the news page.\n",
    "            news = news.find(class_='h1 modul-petit')\n",
    "            link_to_news.append(news.find(\"a\").get('href'))\n",
    "    \n",
    "    return link_to_news\n",
    "\n",
    "def dbc_query():\n",
    "    link_to_news = dbc_get_links()\n",
    "\n",
    "    # Start cloud multiprocessing.\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(dbc_process_news, link_to_news)\n",
    "    count = sum(result)\n",
    "\n",
    "    if count == 0:\n",
    "        print(\"DiarideBarcelona: no results found.\")\n",
    "    else:\n",
    "        print(\"DiarideBarcelona: \"+str(count)+\" results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 - Leave the search engine.\n",
      "1 - Regular search using a topic.\n",
      "2 - Advanced search using filters.\n",
      "3 - Advanced search with filters and data analytics.\n",
      "2021-06-18 00:24:05,481 [INFO] lithops.config -- Lithops v2.3.4\n",
      "2021-06-18 00:24:05,498 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-gb\n",
      "2021-06-18 00:24:05,500 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: us-south - Namespace: lluisoriol.colom@estudiants.urv.cat_dev\n",
      "2021-06-18 00:24:05,543 [INFO] lithops.executors -- Serverless Executor created with ID: 3d75f6-11\n",
      "2021-06-18 00:24:05,565 [INFO] lithops.invokers -- ExecutorID 3d75f6-11 | JobID M000 - Selected Runtime: repstail123/sdpractica2:sdpract2 - 512MB\n",
      "2021-06-18 00:24:05,576 [INFO] lithops.job.job -- ExecutorID 3d75f6-11 | JobID M000 - Uploading function and data - Total: 2.3KiB\n",
      "2021-06-18 00:24:06,361 [INFO] lithops.invokers -- ExecutorID 3d75f6-11 | JobID M000 - Starting function invocation: CloudWorker() - Total: 1 activations\n",
      "2021-06-18 00:24:06,364 [INFO] lithops.invokers -- ExecutorID 3d75f6-11 | JobID M000 - View execution logs at /tmp/lithops/logs/3d75f6-11-M000.log\n",
      "2021-06-18 00:24:06,371 [INFO] lithops.wait -- ExecutorID 3d75f6-11 - Waiting for functions to complete\n",
      "2021-06-18 00:24:06,570 [INFO] lithops.config -- Lithops v2.3.4\n",
      "2021-06-18 00:24:06,590 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-gb\n",
      "2021-06-18 00:24:06,591 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: us-south - Namespace: lluisoriol.colom@estudiants.urv.cat_dev\n",
      "2021-06-18 00:24:06,592 [INFO] lithops.executors -- Serverless Executor created with ID: 3d75f6-12\n",
      "2021-06-18 00:24:06,592 [INFO] lithops.invokers -- ExecutorID 3d75f6-12 | JobID M000 - Selected Runtime: repstail123/sdpractica2:sdpract2 - 512MB\n",
      "2021-06-18 00:24:06,603 [INFO] lithops.job.job -- ExecutorID 3d75f6-12 | JobID M000 - Uploading function and data - Total: 8.4KiB\n",
      "2021-06-18 00:24:07,195 [INFO] lithops.invokers -- ExecutorID 3d75f6-12 | JobID M000 - Starting function invocation: CloudWorker() - Total: 39 activations\n",
      "2021-06-18 00:24:07,240 [INFO] lithops.invokers -- ExecutorID 3d75f6-12 | JobID M000 - View execution logs at /tmp/lithops/logs/3d75f6-12-M000.log\n",
      "2021-06-18 00:24:07,249 [INFO] lithops.wait -- ExecutorID 3d75f6-12 - Waiting for functions to complete\n",
      "2021-06-18 00:24:07,947 [INFO] lithops.config -- Lithops v2.3.4\n",
      "2021-06-18 00:24:07,964 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-gb\n",
      "2021-06-18 00:24:07,965 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: us-south - Namespace: lluisoriol.colom@estudiants.urv.cat_dev\n",
      "2021-06-18 00:24:07,966 [INFO] lithops.executors -- Serverless Executor created with ID: 3d75f6-13\n",
      "2021-06-18 00:24:07,967 [INFO] lithops.invokers -- ExecutorID 3d75f6-13 | JobID M000 - Selected Runtime: repstail123/sdpractica2:sdpract2 - 512MB\n",
      "2021-06-18 00:24:07,973 [INFO] lithops.job.job -- ExecutorID 3d75f6-13 | JobID M000 - Uploading function and data - Total: 3.0KiB\n",
      "2021-06-18 00:24:08,525 [INFO] lithops.invokers -- ExecutorID 3d75f6-13 | JobID M000 - Starting function invocation: CloudWorker() - Total: 4 activations\n",
      "2021-06-18 00:24:08,531 [INFO] lithops.invokers -- ExecutorID 3d75f6-13 | JobID M000 - View execution logs at /tmp/lithops/logs/3d75f6-13-M000.log\n",
      "2021-06-18 00:24:08,537 [INFO] lithops.wait -- ExecutorID 3d75f6-13 - Waiting for functions to complete\n",
      "2021-06-18 00:24:11,380 [INFO] lithops.wait -- ExecutorID 3d75f6-11 - Getting results from functions\n",
      "2021-06-18 00:24:11,454 [INFO] lithops.executors -- ExecutorID 3d75f6-11 - Cleaning temporary data\n",
      "2021-06-18 00:24:15,556 [INFO] lithops.wait -- ExecutorID 3d75f6-13 - Getting results from functions\n",
      "2021-06-18 00:24:15,638 [INFO] lithops.executors -- ExecutorID 3d75f6-13 - Cleaning temporary data\n",
      "DiarideBarcelona: 3 results found.\n",
      "2021-06-18 00:24:17,299 [INFO] lithops.wait -- ExecutorID 3d75f6-12 - Getting results from functions\n",
      "2021-06-18 00:24:17,547 [INFO] lithops.executors -- ExecutorID 3d75f6-12 - Cleaning temporary data\n",
      "2021-06-18 00:24:17,589 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-gb\n",
      "2021-06-18 00:24:18,064 [INFO] lithops.config -- Lithops v2.3.4\n",
      "2021-06-18 00:24:18,082 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-gb\n",
      "2021-06-18 00:24:18,083 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: us-south - Namespace: lluisoriol.colom@estudiants.urv.cat_dev\n",
      "2021-06-18 00:24:18,084 [INFO] lithops.executors -- Serverless Executor created with ID: 3d75f6-14\n",
      "2021-06-18 00:24:18,084 [INFO] lithops.invokers -- ExecutorID 3d75f6-14 | JobID M000 - Selected Runtime: repstail123/sdpractica2:sdpract2 - 512MB\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.lock' object",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2bac554d7037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmenu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"See u soon\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2bac554d7037>\u001b[0m in \u001b[0;36moptions\u001b[0;34m(choice)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprintToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprintToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvancedFilterSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprintToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvancedAnalyticsSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2bac554d7037>\u001b[0m in \u001b[0;36mregularSearch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mnews_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTORAGE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUCKET\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSEARCH_KEY\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mnews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_object_cloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lithops/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lithops/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_map_async\u001b[0;34m(self, func, iterable, chunksize, callback, error_callback, starmap)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mextra_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENV_VARS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMapResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lithops/executors.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_function, map_iterdata, chunksize, worker_processes, extra_args, extra_env, runtime_memory, chunk_size, chunk_n, obj_chunk_size, obj_chunk_number, timeout, invoke_pool_threads, include_modules, exclude_modules)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mruntime_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         job = create_map_job(self.config, self.internal_storage,\n\u001b[0m\u001b[1;32m    230\u001b[0m                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                              \u001b[0mmap_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lithops/job/job.py\u001b[0m in \u001b[0;36mcreate_map_job\u001b[0;34m(config, internal_storage, executor_id, job_id, map_function, iterdata, runtime_meta, runtime_memory, extra_env, include_modules, exclude_modules, execution_timeout, chunksize, worker_processes, extra_args, obj_chunk_size, obj_chunk_number, chunk_size, chunk_n, invoke_pool_threads)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# ########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     job = _create_job(config=config,\n\u001b[0m\u001b[1;32m     73\u001b[0m                       \u001b[0minternal_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minternal_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                       \u001b[0mexecutor_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lithops/job/job.py\u001b[0m in \u001b[0;36m_create_job\u001b[0;34m(config, internal_storage, executor_id, job_id, func, iterdata, runtime_meta, runtime_memory, extra_env, include_modules, exclude_modules, execution_timeout, host_job_meta, chunksize, worker_processes, invoke_pool_threads)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mjob_serialize_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializeIndependent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preinstalls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mfunc_and_data_ser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miterdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_modules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0mdata_strs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_data_ser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mdata_size_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_strs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lithops/job/serialize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, list_of_objs, include_modules, exclude_modules)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_objs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mmods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module_inspect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mstrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Add modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"recursion\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object"
     ]
    }
   ],
   "source": [
    "def menu():\n",
    "    print(\"0 - Leave the search engine.\")\n",
    "    print(\"1 - Regular search using a topic.\")\n",
    "    print(\"2 - Advanced search using filters.\")\n",
    "    print(\"3 - Advanced search with filters and data analytics.\")\n",
    "    return int(input(\"Choice: \"))\n",
    "\n",
    "def get_object_cloud(key):\n",
    "        return json.loads(STORAGE.get_object(BUCKET,key))\n",
    "\n",
    "def regularSearch():\n",
    "    global SEARCH_KEY\n",
    "    global STORAGE\n",
    "    SEARCH_KEY = ''\n",
    "    while SEARCH_KEY == '':\n",
    "        SEARCH_KEY = input(\"Choose the topic: \")\n",
    "    \n",
    "    ccma_thread = Thread(target=ccma_query)\n",
    "    ccma_thread.start()\n",
    "    dtg_thread = Thread(target=dtg_query)\n",
    "    dtg_thread.start()\n",
    "    dbc_thread = Thread(target=dbc_query)\n",
    "    dbc_thread.start()\n",
    "    \n",
    "    ccma_thread.join()\n",
    "    dtg_thread.join()\n",
    "    dbc_thread.join()\n",
    "\n",
    "    STORAGE = Storage()\n",
    "    news_list = STORAGE.list_keys(BUCKET,SEARCH_KEY+'/')\n",
    "    with Pool() as pool:\n",
    "        news = pool.map(get_object_cloud, news_list)\n",
    "    return news\n",
    "    \n",
    "def advancedFilterSearch():\n",
    "    apply_sentiment = input(\"Filter by sentiment analysis? (bias[-1,1] more/less) (blank if none)   \")\n",
    "    apply_wordnumber = input(\"Filter by word numbers (get the news with more/less words than specified)? (bias more/less) (blank if none)   \")\n",
    "    apply_date = input(\"Filter by date (get the news before/after the specified date)? (before/after date[yyyy/mm/dd]) (blank if none)   \")\n",
    "    #apply_specific_word = input(\"Filter by specific word (get the news contaning the word)? (word) (blank if none)   \")\n",
    "    news = regularSearch()\n",
    "\n",
    "    for n in news:\n",
    "        if apply_sentiment.isEmpty() == False:\n",
    "            bias = apply_sentiment[0]\n",
    "            sign = apply_sentiment[1]\n",
    "            if sign == 'more' and float(n['sentiment']) < float(bias):\n",
    "                news.remove(n)\n",
    "                continue\n",
    "\n",
    "            elif sign == 'less' and float(n['sentiment']) > float(bias):\n",
    "                news.remove(n)\n",
    "                continue\n",
    "\n",
    "        if apply_wordnumber.isEmpty() == False:\n",
    "            bias = apply_wordnumber[0]\n",
    "            op = apply_wordnumber[1]\n",
    "            if op == 'more' and int(n['words_number']) < int(bias):\n",
    "                news.remove(n)\n",
    "                continue\n",
    "\n",
    "            elif op == 'less' and int(n['words_number']) > int(bias):\n",
    "                news.remove(n)\n",
    "                continue\n",
    "        \n",
    "        if apply_date.isEmpty() == False:\n",
    "            date = apply_date[1].split(\"/\")\n",
    "            specified_date = datetime.datetime(date[0],date[1],date[2])\n",
    "            news_date = n['date'].split(\"/\")\n",
    "            time_frame = apply_date[0]\n",
    "            if time_frame == 'before' and datetime.datetime(news_date[2],news_date[1],news_date[0]) > specified_date:\n",
    "                news.remove(n)\n",
    "                continue\n",
    "\n",
    "            elif time_frame == 'after' and datetime.datetime(news_date[2],news_date[1],news_date[0]) < specified_date:\n",
    "                news.remove(n)\n",
    "                continue\n",
    "    return news\n",
    "\n",
    "def advancedAnalyticsSearch():\n",
    "    news = advancedFilterSearch()\n",
    "    print(\"analytics\")\n",
    "\n",
    "def printToFile(vector):\n",
    "    with open(\"results.txt\",\"w\") as f:\n",
    "        for news in vector:\n",
    "            f.write(json.dumps(news))\n",
    "\n",
    "def options(choice):\n",
    "    if choice == 1: printToFile(regularSearch())\n",
    "    elif choice == 2: printToFile(advancedFilterSearch())\n",
    "    elif choice == 3: printToFile(advancedAnalyticsSearch())\n",
    "    elif choice == 0: print(\"Leaving...\")\n",
    "    else: print(\"Wrong option.\")\n",
    "          \n",
    "# ------------------------------------------------------------------------------------- #\n",
    "if __name__ == '__main__':\n",
    "    choice = -1\n",
    "    while choice != 0:\n",
    "        choice = menu()\n",
    "        options(choice)\n",
    "    print(\"See u soon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}