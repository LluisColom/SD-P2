{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.3 64-bit"
  },
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping libraries.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Sentiment Analisys libraries.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import mtranslate\n",
    "\n",
    "# System libraries.\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "# Lithops.\n",
    "from threading import Thread\n",
    "from lithops.multiprocessing import Pool\n",
    "from lithops import Storage\n",
    "\n",
    "# Http header.\n",
    "header = { 'user-agent':'Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0' }\n",
    "\n",
    "SEARCH_KEY = ''\n",
    "BUCKET = 'news-bucket'\n",
    "\n",
    "# The higher, more results. The lower, less results.\n",
    "DEFAULT_SEARCH_RESULTS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# WEB SCRAWLER FOR THE WEBSITE WWW.CCMA.CAT   #\n",
    "###############################################\n",
    "def ccma_process_news(link):\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    news_format = {}\n",
    "    try:\n",
    "        r = requests.get(link, headers=header)\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        return 0\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    frame = soup.find(\"div\", class_='span8')\n",
    "    \n",
    "    # Put news link.\n",
    "    news_format['link'] = link\n",
    "\n",
    "    # Get news header.\n",
    "    news_format['title'] = frame.find(\"h1\", class_='titol').text\n",
    "\n",
    "    # Get news starter.\n",
    "    starter = frame.find(\"h2\", class_='entradeta')\n",
    "    if starter is not None:\n",
    "        starter = starter.text\n",
    "    else: starter = ''\n",
    "    news_format['starter'] = starter.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    # Get news date.\n",
    "    news_format['date'] = frame.find(\"time\", class_='data').text.split(\" \")[0]\n",
    "\n",
    "    # Get news paragraphs.\n",
    "    frame = frame.find(\"div\", class_='R-itemNotiCos')\n",
    "    body = ''\n",
    "    for parraph in frame.find_all(\"p\"):\n",
    "        body = body+\" \"+parraph.text\n",
    "    news_format['body'] = body.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    #Get news sentiment analysis.\n",
    "    try:\n",
    "        news_format['sentiment'] = analyzer.polarity_scores(mtranslate.translate(news_format['starter']+'\\n'+news_format['body'],'en','auto'))['compound']\n",
    "    except urllib.error.HTTPError:\n",
    "        return 0\n",
    "        \n",
    "    # Get news total words number.\n",
    "    word_counter = 0\n",
    "    for field in news_format.values():\n",
    "        word_counter += len(str(field).split(\" \"))\n",
    "    news_format['words_number'] = word_counter\n",
    "\n",
    "    # Store the news content to the cloud COS.\n",
    "    storage = Storage()\n",
    "    storage.put_object(bucket='news-bucket', key=SEARCH_KEY+'/ccma/'+news_format['title'].replace(\" \",\"_\")+'.json', body = json.dumps(news_format))\n",
    "\n",
    "    return 1\n",
    "\n",
    "def ccma_get_links():\n",
    "\n",
    "    # Auxiliar variables.\n",
    "    link_to_news = []\n",
    "\n",
    "    # We create HTML parser.\n",
    "    r = requests.get('https://www.ccma.cat/cercador/?text='+SEARCH_KEY+'&profile=noticies&pagina=1', headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Get the number of pages in the website.\n",
    "    pages = soup.find(class_='numeracio')\n",
    "    if pages is None:\n",
    "        pages = 0\n",
    "    else:\n",
    "        pages = pages.text.split(\" \")[3]\n",
    "\n",
    "    # Get the links to the news.\n",
    "    for i in range(int(pages)+1):\n",
    "        r = requests.get('https://www.ccma.cat/cercador/?text='+SEARCH_KEY+'&profile=noticies&pagina='+str(i), headers=header)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        for news in soup.find_all(\"li\", class_='F-llistat-item'):\n",
    "            # We get the link to the news page.\n",
    "            link_to_news.append(\"https://www.ccma.cat\"+news.find(\"a\").get('href'))\n",
    "    \n",
    "    return link_to_news\n",
    "\n",
    "def ccma_query():\n",
    "\n",
    "    link_to_news = ccma_get_links()\n",
    "\n",
    "    # Start cloud multiprocessing.\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(ccma_process_news, link_to_news)\n",
    "    \n",
    "    count = sum(result)\n",
    "\n",
    "    if count == 0:\n",
    "        return \"ccma: no results found.\"\n",
    "    else:\n",
    "        return \"ccma: \"+str(count)+\" results found.\"\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# WEB SCRAWLER FOR THE WEBSITE WWW.DIARIDETARRAGONA.COM   #\n",
    "###########################################################\n",
    "\n",
    "def dtg_process_news(link):\n",
    "     \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    news_format = {}\n",
    "    try:\n",
    "        r = requests.get(link, headers=header)\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        return 0\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    head = soup.find(\"header\", class_='news-header')\n",
    "\n",
    "    # Put news link.\n",
    "    news_format['link'] = link\n",
    "\n",
    "    # Get news header.\n",
    "    news_format['title'] = head.find(\"h1\", class_='news-title').text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    # Get news starter.\n",
    "    starter = head.find(\"div\",class_='news-excerpt')\n",
    "    if starter is not None:\n",
    "        starter = starter.text\n",
    "    else: starter = ''\n",
    "    news_format['starter'] = starter.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    # Get news date.\n",
    "    news_format['date'] = head.find(\"time\", class_='news-date').text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").split(\" \")[0]\n",
    "\n",
    "    # Get news paragraphs.\n",
    "    frame = soup.find(\"div\", class_='news-body')\n",
    "    body = ''\n",
    "    for parraph in frame.find_all(\"p\"):\n",
    "        body = body+\" \"+parraph.text\n",
    "    news_format['body'] = body.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    #Get news sentiment analysis.\n",
    "    try:\n",
    "        news_format['sentiment'] = analyzer.polarity_scores(mtranslate.translate(news_format['starter']+'\\n'+news_format['body'],'en','auto'))['compound']\n",
    "    except urllib.error.HTTPError:\n",
    "        return 0\n",
    "    \n",
    "    # Get news total words number.\n",
    "    word_counter = 0\n",
    "    for field in news_format.values():\n",
    "        word_counter += len(str(field).split(\" \"))\n",
    "    news_format['words_number'] = word_counter\n",
    "\n",
    "    # Store the news content to the cloud COS.\n",
    "    storage = Storage()\n",
    "    storage.put_object(bucket='news-bucket', key=SEARCH_KEY+'/diaridetarragona/'+news_format['title'].replace(\" \",\"_\")+'.json', body = json.dumps(news_format))\n",
    "\n",
    "    return 1\n",
    "\n",
    "def dtg_get_links():\n",
    "\n",
    "    # Auxiliar variables.\n",
    "    link_to_news = []\n",
    "\n",
    "    # We create HTML parser.\n",
    "    r = requests.get('https://www.diaridetarragona.com/ajax/get_search_news.html?viewmore=%2Fajax%2Fget_search_news.html&page=1&size='+str(DEFAULT_SEARCH_RESULTS)+'&search='+SEARCH_KEY, headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Get the links to the news.\n",
    "    for news in soup.find_all(\"div\", class_='news-data'):\n",
    "        # We get the link to the news page.\n",
    "        link_to_news.append(\"https://www.diaridetarragona.com\"+news.find(\"a\").get('href'))\n",
    "\n",
    "    return link_to_news\n",
    "\n",
    "def dtg_query():\n",
    "    \n",
    "    link_to_news = dtg_get_links()\n",
    "\n",
    "    # Start cloud multiprocessing.\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(dtg_process_news, link_to_news)\n",
    "    count = sum(result)\n",
    "    \n",
    "    if count == 0:\n",
    "        return \"DiarideTarragona: no results found.\"\n",
    "    else:\n",
    "        return \"DiarideTarragona: \"+str(count)+\" results found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# WEB SCRAWLER FOR THE WEBSITE WWW.DIARIDEBARCELONA.CAT   #\n",
    "###########################################################\n",
    "\n",
    "def dbc_process_news(link):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    news_format = {}\n",
    "    try:\n",
    "        r = requests.get(link, headers=header)\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        return 0\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # Put news link.\n",
    "    news_format['link'] = link\n",
    "\n",
    "    # Get news header.\n",
    "    title = soup.find(\"div\", class_='title-opening-section')\n",
    "    if title is None:\n",
    "        return 0         #  Case we are threatting an opinion...\n",
    "    news_format['title'] = title.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\\"\",\"\")\n",
    "\n",
    "    # Get news starter.\n",
    "    description = soup.find(\"div\", class_='description')\n",
    "    if description is None:\n",
    "        return 0         # Case we are threatting an interview...\n",
    "    news_format['starter'] = description.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\xa0\",\" \").replace(\"\\\"\",\"\")\n",
    "\n",
    "    # Get news date.\n",
    "    frame = soup.find(\"div\", class_='info-date')\n",
    "    date = frame.find(\"span\", class_='date').text\n",
    "    news_format['date'] = date.replace(\" \",\"\").replace(\"d’\",\"/\").replace(\"de\",\"/\").replace(\"gener\",\"01/\").replace(\"febrer\",\"02/\").replace(\"març\",\"03/\").replace(\"abril\",\"04/\").replace(\"maig\",\"05/\").replace(\"juny\",\"06/\").replace(\"juliol\",\"07/\").replace(\"agost\",\"08/\").replace(\"setembre\",\"09/\").replace(\"octubre\",\"10/\").replace(\"novembre\",\"11/\").replace(\"desembre\",\"12/\")\n",
    "    # Get news paragraphs.\n",
    "    frame = soup.findAll(\"div\", class_='component-html pb-3')[3]\n",
    "    body = ''\n",
    "    for parraph in frame.find_all(\"p\"):\n",
    "        body = body+\" \"+parraph.text\n",
    "    news_format['body'] = body.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\xa0\",\" \").replace(\"\\\"\",\"\")\n",
    "    \n",
    "    #Get news sentiment analysis.\n",
    "    try:\n",
    "        news_format['sentiment'] = analyzer.polarity_scores(mtranslate.translate(news_format['starter']+'\\n'+news_format['body'],'en','auto'))['compound']\n",
    "    except urllib.error.HTTPError:\n",
    "        return 0\n",
    "\n",
    "    # Get news total words number.\n",
    "    word_counter = 0\n",
    "    for field in news_format.values():\n",
    "        word_counter += len(str(field).split(\" \"))\n",
    "    news_format['words_number'] = word_counter\n",
    "\n",
    "    # Store the news content to the cloud COS.\n",
    "    storage = Storage()\n",
    "    storage.put_object(bucket='news-bucket', key=SEARCH_KEY+'/diaridebarcelona/'+news_format['title'].replace(\" \",\"_\")+'.json', body = json.dumps(news_format))\n",
    "\n",
    "    return 1\n",
    "\n",
    "def dbc_get_links():\n",
    "\n",
    "    # Auxiliar variables.\n",
    "    link_to_news = []\n",
    "\n",
    "    # We create HTML parser.\n",
    "    r = requests.get('https://www.diaridebarcelona.cat/search?q='+sys.argv[1], headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Get the number of pages in the website.\n",
    "    frame = soup.find(lambda tag: tag.name == 'li' and tag.get('class') == ['first'])\n",
    "    if frame is not None:\n",
    "        pages = frame.find(\"a\").get('href')\n",
    "        pages = pages.split(\"=\")[-1]\n",
    "    else:\n",
    "        pages = 0  \n",
    "\n",
    "    # Get the links to the news.\n",
    "    for i in range(int(pages)+1):\n",
    "        r = requests.get('https://www.diaridebarcelona.cat/search?q='+SEARCH_KEY+'&start='+str(i), headers=header)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        for news in soup.find_all(class_='col-sm-6 col-lg-3 mb-20px mb-lg-30px'):\n",
    "            # We get the link to the news page.\n",
    "            news = news.find(class_='h1 modul-petit')\n",
    "            link_to_news.append(news.find(\"a\").get('href'))\n",
    "    \n",
    "    return link_to_news\n",
    "\n",
    "def dbc_query():\n",
    "    link_to_news = dbc_get_links()\n",
    "\n",
    "    # Start cloud multiprocessing.\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(dbc_process_news, link_to_news)\n",
    "    count = sum(result)\n",
    "\n",
    "    if count == 0:\n",
    "        print(\"DiarideBarcelona: no results found.\")\n",
    "    else:\n",
    "        print(\"DiarideBarcelona: \"+str(count)+\" results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "#         MAIN AUXILIAR FUNCTIONS         #\n",
    "###########################################\n",
    "\n",
    "def menu():\n",
    "    print(\"0 - Leave the search engine.\")\n",
    "    print(\"1 - Regular search using a topic.\")\n",
    "    print(\"2 - Advanced search using filters.\")\n",
    "    print(\"3 - Advanced search with filters and data analytics.\")\n",
    "    return int(input(\"Choice: \"))\n",
    "\n",
    "def get_object_cloud(key):\n",
    "        stor = Storage()\n",
    "        return json.loads(stor.get_object(BUCKET,key))\n",
    "\n",
    "def regularSearch():\n",
    "    global SEARCH_KEY\n",
    "    SEARCH_KEY = ''\n",
    "    while SEARCH_KEY == '':\n",
    "        SEARCH_KEY = input(\"Choose the topic: \")\n",
    "    \n",
    "    ccma_thread = Thread(target=ccma_query)\n",
    "    ccma_thread.start()\n",
    "    dtg_thread = Thread(target=dtg_query)\n",
    "    dtg_thread.start()\n",
    "    dbc_thread = Thread(target=dbc_query)\n",
    "    dbc_thread.start()\n",
    "    \n",
    "    ccma_thread.join()\n",
    "    dtg_thread.join()\n",
    "    dbc_thread.join()\n",
    "\n",
    "    storage = Storage()\n",
    "    news_list = storage.list_keys(BUCKET,SEARCH_KEY+'/')\n",
    "    with Pool() as pool:\n",
    "        news = pool.map(get_object_cloud, news_list)\n",
    "    return news\n",
    "    \n",
    "def advancedFilterSearch():\n",
    "    apply_sentiment = input(\"Filter by sentiment analysis? (bias[-1,1] more/less) (blank if none)   \")\n",
    "    apply_wordnumber = input(\"Filter by word numbers (get the news with more/less words than specified)? (bias more/less) (blank if none)   \")\n",
    "    apply_date = input(\"Filter by date (get the news before/after the specified date)? (before/after date[yyyy/mm/dd]) (blank if none)   \")\n",
    "    apply_specific_word = input(\"Filter by specific word (get the news contaning the word)? (word) (blank if none)   \")\n",
    "    news = regularSearch()\n",
    "\n",
    "    filtered_news = []\n",
    "    for n in news:\n",
    "        filtered_news.append(n)\n",
    "\n",
    "    for n in news:\n",
    "\n",
    "        if apply_specific_word:\n",
    "            if apply_specific_word not in json.dumps(n):\n",
    "                filtered_news.remove(n)\n",
    "                continue\n",
    "\n",
    "        if apply_sentiment:\n",
    "            bias = apply_sentiment.split(\" \")[0]\n",
    "            sign = apply_sentiment.split(\" \")[1]\n",
    "            if sign == 'more' and float(n['sentiment']) < float(bias):\n",
    "                filtered_news.remove(n)\n",
    "                continue\n",
    "\n",
    "            elif sign == 'less' and float(n['sentiment']) > float(bias):\n",
    "                filtered_news.remove(n)\n",
    "                continue\n",
    "\n",
    "        if apply_wordnumber:\n",
    "            bias = apply_wordnumber.split(\" \")[0]\n",
    "            op = apply_wordnumber.split(\" \")[1]\n",
    "            if op == 'more' and int(n['words_number']) < int(bias):\n",
    "                filtered_news.remove(n)\n",
    "                continue\n",
    "\n",
    "            elif op == 'less' and int(n['words_number']) > int(bias):\n",
    "                filtered_news.remove(n)\n",
    "                continue\n",
    "        \n",
    "        if apply_date:\n",
    "            date = apply_date.split(\" \")[1].split(\"/\")\n",
    "            specified_date = datetime.datetime(int(date[0]),int(date[1]),int(date[2]))\n",
    "            news_date = n['date'].split(\"/\")\n",
    "            time_frame = apply_date.split(\" \")[0]\n",
    "            if time_frame == 'before' and datetime.datetime(int(news_date[2]),int(news_date[1]),int(news_date[0])) > specified_date:\n",
    "                filtered_news.remove(n)\n",
    "                continue\n",
    "\n",
    "            elif time_frame == 'after' and datetime.datetime(int(news_date[2]),int(news_date[1]),int(news_date[0])) < specified_date:\n",
    "                filtered_news.remove(n)\n",
    "                continue\n",
    "    return filtered_news\n",
    "\n",
    "def advancedAnalyticsSearch():\n",
    "    news = advancedFilterSearch()\n",
    "    print(\"analytics\")\n",
    "\n",
    "def printToFile(vector):\n",
    "    with open(\"results.txt\",\"w\") as f:\n",
    "        count = 0\n",
    "        for news in vector:\n",
    "            count += 1\n",
    "            for field in news.values():\n",
    "                f.write(str(field)+\"\\n\")\n",
    "            f.write(\"****************************************************************************\\n\")\n",
    "    print (\"Number of indexed news: \"+str(count))\n",
    "    \n",
    "def options(choice):\n",
    "    if choice == 1: printToFile(regularSearch())\n",
    "    elif choice == 2: printToFile(advancedFilterSearch())\n",
    "    elif choice == 3: printToFile(advancedAnalyticsSearch())\n",
    "    elif choice == 0: print(\"Leaving...\")\n",
    "    else: print(\"Wrong option.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 - Leave the search engine.\n",
      "1 - Regular search using a topic.\n",
      "2 - Advanced search using filters.\n",
      "3 - Advanced search with filters and data analytics.\n",
      "2021-06-18 11:09:06,720 [INFO] lithops.config -- Lithops v2.3.4\n",
      "2021-06-18 11:09:06,735 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-gb\n",
      "2021-06-18 11:09:06,736 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: us-south - Namespace: lluisoriol.colom@estudiants.urv.cat_dev\n",
      "2021-06-18 11:09:06,737 [INFO] lithops.executors -- Serverless Executor created with ID: baa24f-108\n",
      "2021-06-18 11:09:06,738 [INFO] lithops.invokers -- ExecutorID baa24f-108 | JobID M000 - Selected Runtime: repstail123/sdpractica2:sdpract2 - 512MB\n",
      "2021-06-18 11:09:06,743 [INFO] lithops.job.job -- ExecutorID baa24f-108 | JobID M000 - Uploading function and data - Total: 2.3KiB\n",
      "2021-06-18 11:09:07,376 [INFO] lithops.invokers -- ExecutorID baa24f-108 | JobID M000 - Starting function invocation: CloudWorker() - Total: 1 activations\n",
      "2021-06-18 11:09:07,403 [INFO] lithops.invokers -- ExecutorID baa24f-108 | JobID M000 - View execution logs at /tmp/lithops/logs/baa24f-108-M000.log\n",
      "2021-06-18 11:09:07,411 [INFO] lithops.wait -- ExecutorID baa24f-108 - Waiting for functions to complete\n",
      "2021-06-18 11:09:07,637 [INFO] lithops.config -- Lithops v2.3.4\n",
      "2021-06-18 11:09:07,655 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-gb\n",
      "2021-06-18 11:09:07,656 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: us-south - Namespace: lluisoriol.colom@estudiants.urv.cat_dev\n",
      "2021-06-18 11:09:07,658 [INFO] lithops.executors -- Serverless Executor created with ID: baa24f-109\n",
      "2021-06-18 11:09:07,659 [INFO] lithops.invokers -- ExecutorID baa24f-109 | JobID M000 - Selected Runtime: repstail123/sdpractica2:sdpract2 - 512MB\n",
      "2021-06-18 11:09:07,671 [INFO] lithops.job.job -- ExecutorID baa24f-109 | JobID M000 - Uploading function and data - Total: 8.4KiB\n",
      "2021-06-18 11:09:08,321 [INFO] lithops.invokers -- ExecutorID baa24f-109 | JobID M000 - Starting function invocation: CloudWorker() - Total: 39 activations\n",
      "2021-06-18 11:09:08,372 [INFO] lithops.invokers -- ExecutorID baa24f-109 | JobID M000 - View execution logs at /tmp/lithops/logs/baa24f-109-M000.log\n",
      "2021-06-18 11:09:08,380 [INFO] lithops.wait -- ExecutorID baa24f-109 - Waiting for functions to complete\n",
      "2021-06-18 11:09:08,836 [INFO] lithops.config -- Lithops v2.3.4\n",
      "2021-06-18 11:09:08,855 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-gb\n",
      "2021-06-18 11:09:08,856 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: us-south - Namespace: lluisoriol.colom@estudiants.urv.cat_dev\n",
      "2021-06-18 11:09:08,857 [INFO] lithops.executors -- Serverless Executor created with ID: baa24f-110\n",
      "2021-06-18 11:09:08,857 [INFO] lithops.invokers -- ExecutorID baa24f-110 | JobID M000 - Selected Runtime: repstail123/sdpractica2:sdpract2 - 512MB\n",
      "2021-06-18 11:09:08,864 [INFO] lithops.job.job -- ExecutorID baa24f-110 | JobID M000 - Uploading function and data - Total: 3.0KiB\n",
      "2021-06-18 11:09:09,448 [INFO] lithops.invokers -- ExecutorID baa24f-110 | JobID M000 - Starting function invocation: CloudWorker() - Total: 4 activations\n",
      "2021-06-18 11:09:09,454 [INFO] lithops.invokers -- ExecutorID baa24f-110 | JobID M000 - View execution logs at /tmp/lithops/logs/baa24f-110-M000.log\n",
      "2021-06-18 11:09:09,460 [INFO] lithops.wait -- ExecutorID baa24f-110 - Waiting for functions to complete\n",
      "2021-06-18 11:09:12,419 [INFO] lithops.wait -- ExecutorID baa24f-108 - Getting results from functions\n",
      "2021-06-18 11:09:12,507 [INFO] lithops.executors -- ExecutorID baa24f-108 - Cleaning temporary data\n",
      "2021-06-18 11:09:17,476 [INFO] lithops.wait -- ExecutorID baa24f-110 - Getting results from functions\n",
      "2021-06-18 11:09:17,710 [INFO] lithops.executors -- ExecutorID baa24f-110 - Cleaning temporary data\n",
      "DiarideBarcelona: 3 results found.\n",
      "2021-06-18 11:09:18,446 [INFO] lithops.wait -- ExecutorID baa24f-109 - Getting results from functions\n",
      "2021-06-18 11:09:18,692 [INFO] lithops.executors -- ExecutorID baa24f-109 - Cleaning temporary data\n",
      "2021-06-18 11:09:18,759 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-gb\n",
      "2021-06-18 11:09:19,194 [INFO] lithops.config -- Lithops v2.3.4\n",
      "2021-06-18 11:09:19,210 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS Storage client created - Region: eu-gb\n",
      "2021-06-18 11:09:19,211 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: us-south - Namespace: lluisoriol.colom@estudiants.urv.cat_dev\n",
      "2021-06-18 11:09:19,212 [INFO] lithops.executors -- Serverless Executor created with ID: baa24f-111\n",
      "2021-06-18 11:09:19,213 [INFO] lithops.invokers -- ExecutorID baa24f-111 | JobID M000 - Selected Runtime: repstail123/sdpractica2:sdpract2 - 512MB\n",
      "2021-06-18 11:09:19,222 [INFO] lithops.job.job -- ExecutorID baa24f-111 | JobID M000 - Uploading function and data - Total: 6.7KiB\n",
      "2021-06-18 11:09:19,827 [INFO] lithops.invokers -- ExecutorID baa24f-111 | JobID M000 - Starting function invocation: CloudWorker() - Total: 43 activations\n",
      "2021-06-18 11:09:19,873 [INFO] lithops.invokers -- ExecutorID baa24f-111 | JobID M000 - View execution logs at /tmp/lithops/logs/baa24f-111-M000.log\n",
      "2021-06-18 11:09:19,884 [INFO] lithops.wait -- ExecutorID baa24f-111 - Waiting for functions to complete\n",
      "2021-06-18 11:09:28,953 [INFO] lithops.wait -- ExecutorID baa24f-111 - Getting results from functions\n",
      "2021-06-18 11:09:29,212 [INFO] lithops.executors -- ExecutorID baa24f-111 - Cleaning temporary data\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-d1ba54b542f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmenu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"See u soon\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-bd66f0cc4de8>\u001b[0m in \u001b[0;36moptions\u001b[0;34m(choice)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprintToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprintToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvancedFilterSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprintToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvancedAnalyticsSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Leaving...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-bd66f0cc4de8>\u001b[0m in \u001b[0;36madvancedFilterSearch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapply_wordnumber\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_wordnumber\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_wordnumber\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'more'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words_number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mfiltered_news\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#         MAIN          #\n",
    "#########################\n",
    "if __name__ == '__main__':\n",
    "    choice = -1\n",
    "    while choice != 0:\n",
    "        choice = menu()\n",
    "        options(choice)\n",
    "    print(\"See u soon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}