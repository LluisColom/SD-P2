{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.3 64-bit"
  },
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping libraries.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Sentiment Analisys libraries.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import mtranslate\n",
    "\n",
    "# System libraries.\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "# Lithops.\n",
    "from threading import Thread\n",
    "from lithops.multiprocessing import Pool\n",
    "from lithops import Storage\n",
    "\n",
    "# Http header.\n",
    "header = { 'user-agent':'Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0' }\n",
    "\n",
    "SEARCH_KEY = ''\n",
    "\n",
    "BUCKET = 'news-bucket'\n",
    "\n",
    "# The higher, more results. The lower, less results.\n",
    "DEFAULT_SEARCH_RESULTS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# WEB SCRAWLER FOR THE WEBSITE WWW.CCMA.CAT   #\n",
    "###############################################\n",
    "def ccma_process_news(link):\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    news_format = {}\n",
    "    try:\n",
    "        r = requests.get(link, headers=header)\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        return 0\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    frame = soup.find(\"div\", class_='span8')\n",
    "    \n",
    "    # Put news link.\n",
    "    news_format['link'] = link\n",
    "\n",
    "    # Get news header.\n",
    "    news_format['title'] = frame.find(\"h1\", class_='titol').text\n",
    "\n",
    "    # Get news starter.\n",
    "    starter = frame.find(\"h2\", class_='entradeta')\n",
    "    if starter is not None:\n",
    "        starter = starter.text\n",
    "    else: starter = ''\n",
    "    news_format['starter'] = starter.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    # Get news date.\n",
    "    news_format['date'] = frame.find(\"time\", class_='data').text.split(\" \")[0]\n",
    "\n",
    "    # Get news paragraphs.\n",
    "    frame = frame.find(\"div\", class_='R-itemNotiCos')\n",
    "    body = ''\n",
    "    for parraph in frame.find_all(\"p\"):\n",
    "        body = body+\" \"+parraph.text\n",
    "    news_format['body'] = body.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    #Get news sentiment analysis.\n",
    "    try:\n",
    "        news_format['sentiment'] = analyzer.polarity_scores(mtranslate.translate(news_format['starter']+'\\n'+news_format['body'],'en','auto'))['compound']\n",
    "    except urllib.error.HTTPError:\n",
    "        return 0\n",
    "        \n",
    "    # Get news total words number.\n",
    "    word_counter = 0\n",
    "    for field in news_format.values():\n",
    "        word_counter += len(str(field).split(\" \"))\n",
    "    news_format['words_number'] = word_counter\n",
    "\n",
    "    # Store the news content to the cloud COS.\n",
    "    storage = Storage()\n",
    "    storage.put_object(bucket='news-bucket', key=SEARCH_KEY+'/ccma/'+news_format['title'].replace(\" \",\"_\")+'.json', body = json.dumps(news_format))\n",
    "\n",
    "    return 1\n",
    "\n",
    "def ccma_get_links():\n",
    "\n",
    "    # Auxiliar variables.\n",
    "    link_to_news = []\n",
    "\n",
    "    # We create HTML parser.\n",
    "    r = requests.get('https://www.ccma.cat/cercador/?text='+SEARCH_KEY+'&profile=noticies&pagina=1', headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Get the number of pages in the website.\n",
    "    pages = soup.find(class_='numeracio')\n",
    "    if pages is None:\n",
    "        pages = 0\n",
    "    else:\n",
    "        pages = pages.text.split(\" \")[3]\n",
    "\n",
    "    # Get the links to the news.\n",
    "    for i in range(int(pages)+1):\n",
    "        r = requests.get('https://www.ccma.cat/cercador/?text='+SEARCH_KEY+'&profile=noticies&pagina='+str(i), headers=header)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        for news in soup.find_all(\"li\", class_='F-llistat-item'):\n",
    "            # We get the link to the news page.\n",
    "            link_to_news.append(\"https://www.ccma.cat\"+news.find(\"a\").get('href'))\n",
    "    \n",
    "    return link_to_news\n",
    "\n",
    "def ccma_query():\n",
    "\n",
    "    link_to_news = ccma_get_links()\n",
    "\n",
    "    # Start cloud multiprocessing.\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(ccma_process_news, link_to_news)\n",
    "    \n",
    "    count = sum(result)\n",
    "\n",
    "    if count == 0:\n",
    "        return \"ccma: no results found.\"\n",
    "    else:\n",
    "        return \"ccma: \"+str(count)+\" results found.\"\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# WEB SCRAWLER FOR THE WEBSITE WWW.DIARIDETARRAGONA.COM   #\n",
    "###########################################################\n",
    "\n",
    "def dtg_process_news(link):\n",
    "     \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    news_format = {}\n",
    "    try:\n",
    "        r = requests.get(link, headers=header)\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        return 0\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    head = soup.find(\"header\", class_='news-header')\n",
    "\n",
    "    # Put news link.\n",
    "    news_format['link'] = link\n",
    "\n",
    "    # Get news header.\n",
    "    news_format['title'] = head.find(\"h1\", class_='news-title').text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    # Get news starter.\n",
    "    starter = head.find(\"div\",class_='news-excerpt')\n",
    "    if starter is not None:\n",
    "        starter = starter.text\n",
    "    else: starter = ''\n",
    "    news_format['starter'] = starter.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    # Get news date.\n",
    "    news_format['date'] = head.find(\"time\", class_='news-date').text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").split(\" \")[0]\n",
    "\n",
    "    # Get news paragraphs.\n",
    "    frame = soup.find(\"div\", class_='news-body')\n",
    "    body = ''\n",
    "    for parraph in frame.find_all(\"p\"):\n",
    "        body = body+\" \"+parraph.text\n",
    "    news_format['body'] = body.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "\n",
    "    #Get news sentiment analysis.\n",
    "    try:\n",
    "        news_format['sentiment'] = analyzer.polarity_scores(mtranslate.translate(news_format['starter']+'\\n'+news_format['body'],'en','auto'))['compound']\n",
    "    except urllib.error.HTTPError:\n",
    "        return 0\n",
    "    \n",
    "    # Get news total words number.\n",
    "    word_counter = 0\n",
    "    for field in news_format.values():\n",
    "        word_counter += len(str(field).split(\" \"))\n",
    "    news_format['words_number'] = word_counter\n",
    "\n",
    "    # Store the news content to the cloud COS.\n",
    "    storage = Storage()\n",
    "    storage.put_object(bucket='news-bucket', key=SEARCH_KEY+'/diaridetarragona/'+news_format['title'].replace(\" \",\"_\")+'.json', body = json.dumps(news_format))\n",
    "\n",
    "    return 1\n",
    "\n",
    "def dtg_get_links():\n",
    "\n",
    "    # Auxiliar variables.\n",
    "    link_to_news = []\n",
    "\n",
    "    # We create HTML parser.\n",
    "    r = requests.get('https://www.diaridetarragona.com/ajax/get_search_news.html?viewmore=%2Fajax%2Fget_search_news.html&page=1&size='+str(DEFAULT_SEARCH_RESULTS)+'&search='+SEARCH_KEY, headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Get the links to the news.\n",
    "    for news in soup.find_all(\"div\", class_='news-data'):\n",
    "        # We get the link to the news page.\n",
    "        link_to_news.append(\"https://www.diaridetarragona.com\"+news.find(\"a\").get('href'))\n",
    "\n",
    "    return link_to_news\n",
    "\n",
    "def dtg_query():\n",
    "    \n",
    "    link_to_news = dtg_get_links()\n",
    "\n",
    "    # Start cloud multiprocessing.\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(dtg_process_news, link_to_news)\n",
    "    count = sum(result)\n",
    "    \n",
    "    if count == 0:\n",
    "        return \"DiarideTarragona: no results found.\"\n",
    "    else:\n",
    "        return \"DiarideTarragona: \"+str(count)+\" results found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbc_process_news(link):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    news_format = {}\n",
    "    try:\n",
    "        r = requests.get(link, headers=header)\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        return 0\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # Put news link.\n",
    "    news_format['link'] = link\n",
    "\n",
    "    # Get news header.\n",
    "    title = soup.find(\"div\", class_='title-opening-section')\n",
    "    if title is None:\n",
    "        return 0         #  Case we are threatting an opinion...\n",
    "    news_format['title'] = title.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\\"\",\"\")\n",
    "\n",
    "    # Get news starter.\n",
    "    description = soup.find(\"div\", class_='description')\n",
    "    if description is None:\n",
    "        return 0         # Case we are threatting an interview...\n",
    "    news_format['starter'] = description.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\xa0\",\" \").replace(\"\\\"\",\"\")\n",
    "\n",
    "    # Get news date.\n",
    "    frame = soup.find(\"div\", class_='info-date')\n",
    "    date = frame.find(\"span\", class_='date').text\n",
    "    news_format['date'] = date.replace(\" \",\"\").replace(\"d’\",\"/\").replace(\"de\",\"/\").replace(\"gener\",\"01/\").replace(\"febrer\",\"02/\").replace(\"març\",\"03/\").replace(\"abril\",\"04/\").replace(\"maig\",\"05/\").replace(\"juny\",\"06/\").replace(\"juliol\",\"07/\").replace(\"agost\",\"08/\").replace(\"setembre\",\"09/\").replace(\"octubre\",\"10/\").replace(\"novembre\",\"11/\").replace(\"desembre\",\"12/\")\n",
    "    # Get news paragraphs.\n",
    "    frame = soup.findAll(\"div\", class_='component-html pb-3')[3]\n",
    "    body = ''\n",
    "    for parraph in frame.find_all(\"p\"):\n",
    "        body = body+\" \"+parraph.text\n",
    "    news_format['body'] = body.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\xa0\",\" \").replace(\"\\\"\",\"\")\n",
    "    \n",
    "    #Get news sentiment analysis.\n",
    "    try:\n",
    "        news_format['sentiment'] = analyzer.polarity_scores(mtranslate.translate(news_format['starter']+'\\n'+news_format['body'],'en','auto'))['compound']\n",
    "    except urllib.error.HTTPError:\n",
    "        return 0\n",
    "\n",
    "    # Get news total words number.\n",
    "    word_counter = 0\n",
    "    for field in news_format.values():\n",
    "        word_counter += len(str(field).split(\" \"))\n",
    "    news_format['words_number'] = word_counter\n",
    "\n",
    "    # Store the news content to the cloud COS.\n",
    "    storage = Storage()\n",
    "    storage.put_object(bucket='news-bucket', key=SEARCH_KEY+'/diaridebarcelona/'+news_format['title'].replace(\" \",\"_\")+'.json', body = json.dumps(news_format))\n",
    "\n",
    "    return 1\n",
    "\n",
    "def dbc_get_links():\n",
    "\n",
    "    # Auxiliar variables.\n",
    "    link_to_news = []\n",
    "\n",
    "    # We create HTML parser.\n",
    "    r = requests.get('https://www.diaridebarcelona.cat/search?q='+sys.argv[1], headers=header)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Get the number of pages in the website.\n",
    "    frame = soup.find(lambda tag: tag.name == 'li' and tag.get('class') == ['first'])\n",
    "    if frame is not None:\n",
    "        pages = frame.find(\"a\").get('href')\n",
    "        pages = pages.split(\"=\")[-1]\n",
    "    else:\n",
    "        pages = 0  \n",
    "\n",
    "    # Get the links to the news.\n",
    "    for i in range(int(pages)+1):\n",
    "        r = requests.get('https://www.diaridebarcelona.cat/search?q='+SEARCH_KEY+'&start='+str(i), headers=header)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        for news in soup.find_all(class_='col-sm-6 col-lg-3 mb-20px mb-lg-30px'):\n",
    "            # We get the link to the news page.\n",
    "            news = news.find(class_='h1 modul-petit')\n",
    "            link_to_news.append(news.find(\"a\").get('href'))\n",
    "    \n",
    "    return link_to_news\n",
    "\n",
    "def dbc_query():\n",
    "    link_to_news = dbc_get_links()\n",
    "\n",
    "    # Start cloud multiprocessing.\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(dbc_process_news, link_to_news)\n",
    "    count = sum(result)\n",
    "\n",
    "    if count == 0:\n",
    "        print(\"DiarideBarcelona: no results found.\")\n",
    "    else:\n",
    "        print(\"DiarideBarcelona: \"+str(count)+\" results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def menu():\n",
    "    print(\"0 - Leave the search engine.\")\n",
    "    print(\"1 - Regular search using a topic.\")\n",
    "    print(\"2 - Advanced search using filters.\")\n",
    "    print(\"3 - Advanced search with filters and data analytics.\")\n",
    "    return int(input(\"Choice: \"))\n",
    "\n",
    "def get_object_cloud(key):\n",
    "        return json.loads(storage.get_object(BUCKET,key))\n",
    "\n",
    "def regularSearch():\n",
    "    global SEARCH_KEY\n",
    "    SEARCH_KEY = ''\n",
    "    while SEARCH_KEY == '':\n",
    "        SEARCH_KEY =  input(\"Choose the topic: \")\n",
    "    \n",
    "    ccma_thread = Thread(target=ccma_query)\n",
    "    ccma_thread.start()\n",
    "    dtg_thread = Thread(target=dtg_query)\n",
    "    dtg_thread.start()\n",
    "    dbc_thread = Thread(target=dbc_query)\n",
    "    dbc_thread.start()\n",
    "    \n",
    "    ccma_thread.join()\n",
    "    dtg_thread.join()\n",
    "    dbc_thread.join()\n",
    "\n",
    "    storage = Storage()\n",
    "    news_list = storage.list_keys(BUCKET,SEARCH_KEY+'/')\n",
    "    with Pool() as pool:\n",
    "        news = pool.map(get_object_cloud, news_list)\n",
    "    return news\n",
    "    \n",
    "def advancedFilterSearch():\n",
    "    news = regularSearch()\n",
    "    print(\"filter\")\n",
    "\n",
    "def advancedAnalyticsSearch():\n",
    "    news = regularSearch()\n",
    "    print(\"analytics\")\n",
    "\n",
    "def options(choice):\n",
    "    if choice == 1: regularSearch()\n",
    "    elif choice == 2: advancedFilterSearch()\n",
    "    elif choice == 3: advancedAnalyticsSearch()\n",
    "    elif choice == 0: print(\"Leaving...\")\n",
    "    else: print(\"Wrong option.\")\n",
    "          \n",
    "# ------------------------------------------------------------------------------------- #\n",
    "if __name__ == '__main__':\n",
    "    choice = -1\n",
    "    while choice != 0:\n",
    "        choice = menu()\n",
    "        options(choice)\n",
    "    print(\"See u soon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}